---
title: "Practical Machine Learning Project"
author: "PDBolier"
date: "26..27 oktober 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preamble

Based on the data from the website http://groupware.les.inf.puc-rio.br/har we want to predict which excerices were performed. The data is generated by using accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

By analysing the given data a model is build with the goal to predict the manner in which the excercises were performed. 

Part of our approach is to use cross validation, so we follow the steps:

* use the training set
* split the training set into a training and test set
* build a model on the training set
* evaluate on the test set (which was split od the training set)
* repeat ...

However, first we need to:

* have a specific question: which exercise was the participant performing
* have a look at the input data
* extract features
* derive at an algorithm
* find best parameters
* evaluate and reach a conclusion.

### Notice
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har, they have been very generous in allowing their data to be used for this kind of assignment.


## Loading the data
The raw data is generated as described on the site http://groupware.les.inf.puc-rio.br/har.

To decrease the downloads, the training set (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and test set (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) were first downloaded manually. Then the data is read into a dataframe.

Note that we found, using summary, that some fields contain a text "#DIV/0!". This values is intepreted as 'NA'. Several iteration were used to get a cleaner data set.

```{r loaddata}

# Note na.strings 
pml.training.set <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!", "", " "))
pml.testing.set  <- read.csv("pml-testing.csv" , header = TRUE, na.strings = c("NA", "#DIV/0!", "", " "))

```


## Exploration of the data

First we have a quick peek at the training data
```{r peekdata}
# size
nrow(pml.training.set)
ncol(pml.training.set)

# summary(pml.training.set) # too much to show.
# head(plm.training.set) # too much to show

```

Looking at a summary (not shown here) we (assume) that we can remove the following fields:

* 'X' is a row counter so it does not add any important information for this analysis
* user_name, is the user name, also assumed to be not important for this analysis.
* new_window, indicating a new measurment window is started

Additionally (as a first attempt) we also drop the columns with a lot of NA values (more than 80%)

If the test (later on) shows that our model is bad we may reconsider.

When building the model we must take the following aspect into account:

* raw_timestamp_part_1, is (part of) a timestamp, so we will not use it for building the model (yet)
* raw_timestamp_part_2, ..
* cvtd_timestamp, ..
* num_window, part of time related administration (or so it seems)
* classe, which is the outcome, or dependent variable.

Since there are timestamps used there may be a time relation in the data and thus the processing should be in chunks of the data. For a first approach we will ignore the timestamps.  

```{r cleaning}
library(caret)
#library(doMC) # multi core, only on Linux
library(doParallel)

# drop fields X, user_name and those with more than 80% NA's
dropNAcols <- colnames(pml.training.set)[colSums(is.na(pml.training.set))/nrow(pml.training.set) > 0.8] # > 80% NA's
dropcols   <- c(dropNAcols,  c("X", "user_name", "new_window"))

trainingsubcol <- pml.training.set[, !(names(pml.training.set) %in% dropcols), drop=FALSE]
ncol(trainingsubcol)
#summary(training) # shows too much data

# Quick check for type of data
#sapply(training, class) # check type of data

# Dont forget to prepare both training and test set
testingsubcol <- pml.testing.set[, !(names(pml.testing.set) %in% dropcols), drop=FALSE]
ncol(testingsubcol)

```

## split up the training set

```{r splittrain}
set.seed(20161027)
inTraining <- createDataPartition(y = trainingsubcol$classe, p = 0.60, list = FALSE)
training.train <- trainingsubcol[ inTraining, ] # select rows as generated
training.test  <- trainingsubcol[-inTraining, ] # select rows the opposite set

# we keep the testingsubcol set for later
```

## Quick look

Just to get a somewhat better idea, we make a featerplot

```{r featureplot}

# columns 1..4 and the last one are not use, 57 is the outcome.
preprocessObj   <- preProcess(training.train[,c(-(1:4), -57)], method=c("center", "scale"))
trainingPrepped <- predict(preprocessObj, training.train[,c(-(1:4), -57)]) # drop uninteresting columns
trainingPrepped$classe <- training.train$classe

# TODO enlarge plot somehow.
featurePlot(x=trainingPrepped[,-53], y=trainingPrepped$classe) # now 53 is the outcome

# dont forget to prep the test set
testingPrepped <- predict(preprocessObj, training.test[,c(-(1:4), -57)]) # drop uninteresting columns
testingPrepped$classe <- training.test$classe

# prep the 20 cases
test20cases <- predict(preprocessObj, testingsubcol[,c(-(1:4))]) # drop uninteresting columns

```

Well (as one might expect) the plot does not reveal a lot but does show there are differences for the feature (classe). For a better look we may plot less feature (after determining the most important).


## Building the model(s)

Since we only have a set of outcomes (A..E), we're trying to build a classification model. A basic classificaton model can be build using rpart.
From the model we check which predictors are used of the 52 we have.

```{r model1rpart}
set.seed(20161027)

rpart.control  = trainControl(method = "repeatedCV", number = 10, repeats = 10, classProbs = TRUE, verboseIter = FALSE, savePredictions = TRUE)
modelfit.rpart = train (classe ~ . , trainingPrepped, method = "rpart", tuneLength=20, trControl=rpart.control)

# Lets see the predictors
length(predictors(modelfit.rpart)) # how many
predictors(modelfit.rpart) # which ones.

# so how does the model look like?
library(rattle)
fancyRpartPlot(model = modelfit.rpart$finalModel) 

# we might check for the importance
modelfit.rpart$finalModel$variable.importance

# printcp(modelfit.rpart$finalModel) too much, (http://stackoverflow.com/questions/9666212/how-to-compute-error-rate-from-a-decision-tree)
# modelfit.rpart$finalModel[[1]] # shows the accuray / tuned parameters 

# Wanted to use lasso to determine predictors, took too long ... mmm.

```


The 'rpart' model so far uses a 48 preditors, almost all. The tree is too complex to get a proper idea.

The printcp output shows Root node error: 14042/19622 = 0.71563 and for the last split a relative error of  0.28557 resulting in a resubstitution error of  0.2043625 (about 20%), mmm. 

Lets see how the model performs, using the test set, since we have a classification challenge we can use a confusion matrix to check the performance.

```{r testrpart}

predictions.rpart <- predict(modelfit.rpart$finalModel, testingPrepped, type = "class")
confusionMatrix(predictions.rpart, testingPrepped$classe)

```

So the rpart model reaches an accuracy of 0.7838, i.e. 78.38%. Which seems ok, but can we do better.


For a second model we use a random forest model, and for the fird we use a neural net; lets see what results it will bring us.

Note: I wanted to use a boosted model (gbm) but after an hour I had to stop.

```{r model2and3}
library(randomForest)

# this one takes too much time: modelfit.gbm <- train(classe ~ . , data = trainingPrepped, method="gbm", verbose=FALSE)

modelfit.rf <- randomForest(classe ~ ., data = trainingPrepped, method="rf")
predictions.rf <- predict(modelfit.rf, testingPrepped, type = "class")
confusionMatrix(predictions.rf, testingPrepped$classe)

# Last and 3rd model
modelfit.nnet <- train(classe  ~ .,  data = trainingPrepped, method = "nnet", trControl = trainControl(method = "CV", number = 10), linear.output=FALSE, trace=FALSE)
predictions.nnet <- predict(modelfit.nnet, testingPrepped, type = "raw")
confusionMatrix(predictions.nnet, testingPrepped$classe)


```

So the randomforest model shows of  0.9949, i.e. 99.49 %
The confusionmatrix look much, much better (almost no misclassification) then the confusionmatrix of rpart,

And the neural net shows an accuracy of 0.7343, i.e. 73.43%, not good.

The random forest model looks the best (and seems to excute the fastest (TODO measure timing))


## Testing 20 use cases
The testset with the 20 testcases will be used to see if our model is capable of predicting most cases...

```{r predict20}

# predict the 20 testcases
predict(modelfit.rf, test20cases)

```


## Conclusions
The random forest model is the best model for this particular data set.

Unfortunately most steps took quite a lot of computation time, even though I attempted to use multicore processing. Even with 8 cores the modelling took a lot of time while the core were not at all fully used for computation.


